# ğŸ“š Henry Zhang's Paper Reading Logs

Welcome to my personal repository for recording and summarizing research papers I've read.  
My focus areas include:
- **Vision-Language Models (VLMs)**
- **Reinforcement Learning for Multimodal Reasoning**
- **Mixture-of-Experts (MoE)**

---

## ğŸ—‚ï¸ Table of Contents
- [Why Reading Log?](#why-reading-log)
- [ğŸ“… Reading Log](#-reading-log)
- [ğŸ§± Template for New Logs](#-template-for-new-logs)
- [ğŸ§© Legend](#-legend)

---

## Why Reading Log?

As mentioned in [this insightful post](https://www.xiaohongshu.com/discovery/item/68fc7fa90000000003035c7e?source=webshare):  
> â€œ**Scientific research begins with reading papers.**â€  
> Typically, each research area has **10â€“20 core papers** that should be studied thoroughly,  
> and around **40â€“50 important papers** that deserve focused reading.  

Following this philosophy â€” and guided by GPT-5â€™s recommendations â€”  
I maintain this structured log to record my progress across different stages of research reading and exploration.

What's more, I hope these reading records can serve as a useful reference and help fellow newcomers navigate the landscape of research reading more effectively ğŸš€ ğŸš€ ğŸš€ .

---

## ğŸ“… Reading Log

| No. | Paper | Year | Keywords / Main Contribution | Status |
|:---:|:------|:----:|:-----------------------------|:--------|
| 1 | **Attention Is All You Need** | 2017 | Proposed the <mark>Transformer architecture<mark> | âœ… Done Â· 2025-10-29 |
| 2 | **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** | 2018 | Introduced bidirectional masked language modeling and the <mark>pre-training + fine-tuning paradigm<mark>. | âœ… Done Â· 2025-10-30 |
| 3 | **RoBERTa: A Robustly Optimized BERT Pretraining Approach** | 2019 | Demonstrated that <mark>â€œmore data and better recipesâ€<mark> can significantly improve BERTâ€™s performance. | âœ… Done Â· 2025-10-30 |
| 4 | **ALBERT: A Lite BERT for Self-supervised Learning of Language Representations** | 2019 | Achieved lightweight scalability through <mark>parameter sharing and embedding factorization<mark>. | âœ… Done Â· 2025-10-30 |
| 5 | **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation** | 2019 |  | ğŸ“– Planned |
| 6 | **T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer** | 2020 |  | ğŸ“– Planned |
| 7 | **GPT-3: Language Models are Few-Shot Learners** | 2020 |  | ğŸ“– Planned |
| 8 | **Scaling Laws for Neural Language Models** | 2020 |  | ğŸ“– Planned |
| 9 | **Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism** | 2020 |  |  |
| 10 | **Switch Transformer: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity** | 2021 |  |  |
| 11 | **Chinchilla: Training Compute-Optimal Large Language Models** | 2022 |  |  |
| 12 | **PaLM: Scaling Language Modeling with Pathways** | 2022 |  |  |
| 13 | **InstructGPT: Training Language Models to Follow Instructions with Human Feedback** | 2022 |  |  |
| 14 | **FLAN / Flan-T5: Scaling Instruction-Finetuned Language Models** | 2022 |  |  |
| 15 | **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** | 2022 |  |  |
| 16 | **Constitutional AI: Harmlessness from AI Feedback** | 2022 |  |  |
| 17 | **GPT-4 Technical Report** | 2023 |  |  |
| 18 | **LLaMA / LLaMA 2: Open and Efficient Foundation Language Models** | 2023 |  |  |
| 19 | **Mixtral: Sparse Mixture-of-Experts Model** | 2024 |  |  |
| 20 | **LLaMA 3 Series** | 2024 |  |  |

---

## ğŸ§± Template for New Logs

| No. | Paper | Year | Keywords / Main Contribution | Status |
|:---:|:------|:----:|:-----------------------------|:--------|
| XX | **[Paper Title]** | [Year] | [Brief summary, <mark>core ideas<mark>, or keywords] | âœ… Done / ğŸ“– Planned / ğŸ•’ Reading + Date |

```
# how to highlight
<mark>highlight text<mark>
```

## ğŸ§© Legend

| Symbol | Meaning |
|:-------|:---------|
| âœ… Done | Fully read and summarized |
| ğŸ“– Planned | Planned for future reading |
| ğŸ”‘ Core | Foundational or highly influential paper |
| ğŸ§© Topic | Thematically related or supporting work |
| âš™ï¸ Efficiency | Lightweight or optimization-focused paper |
| ğŸš€ Focus | Central to my ongoing research direction |



