# ğŸ“š Henry Zhang's Paper Reading Logs

Welcome to my personal repository for recording and summarizing research papers I've read.  
My focus areas include:
- **Vision-Language Models (VLMs)**
- **Reinforcement Learning for Multimodal Reasoning**
- **Mixture-of-Experts (MoE)**

---

## ğŸ—‚ï¸ Table of Contents
- [Why Reading Log?](#why-reading-log)
- [ğŸ“… Reading Log](#-reading-log)
- [ğŸ§± Template for New Logs](#-template-for-new-logs)
- [ğŸ§© Legend](#-legend)

---

## Why Reading Log?

As mentioned in [this insightful post](https://www.xiaohongshu.com/discovery/item/68fc7fa90000000003035c7e?source=webshare):  
> â€œ**Scientific research begins with reading papers.**â€  
> Typically, each research area has **10â€“20 core papers** that should be studied thoroughly,  
> and around **40â€“50 important papers** that deserve focused reading.  

Following this philosophy â€” and guided by GPT-5â€™s recommendations â€”  
I maintain this structured log to record my progress across different stages of research reading and exploration.

What's more, I hope these reading records can serve as a useful reference and help fellow newcomers navigate the landscape of research reading more effectively ğŸš€ ğŸš€ ğŸš€ .

---

## ğŸ“… Reading Log

| No. | Paper | Year | Keywords / Main Contribution | Status |
|:---:|:------|:----:|:-----------------------------|:--------|
| 1 | **Attention Is All You Need**ğŸ”‘ | 2017 | Proposed the <mark>Transformer architecture<mark> | âœ… Done Â· 2025-10-29  |
| 2 | **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**ğŸ”‘ Core | 2018 | Introduced bidirectional masked language modeling and the <mark>pre-training + fine-tuning paradigm<mark>. | âœ… Done Â· 2025-10-30 |
| 3 | **RoBERTa: A Robustly Optimized BERT Pretraining Approach** | 2019 | Demonstrated that <mark>â€œmore data and better recipesâ€<mark> can significantly improve BERTâ€™s performance. | âœ… Done Â· 2025-10-30 |
| 4 | **ALBERT: A Lite BERT for Self-supervised Learning of Language Representations** | 2019 | Achieved lightweight scalability through <mark>parameter sharing and embedding factorization<mark>. | âœ… Done Â· 2025-10-30 |
| 5 | **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation** | 2019 | BERT+GPTçš„ç»“æ„ï¼Œå¹¶åœ¨encoderç«¯å°è¯•å¤šç§noiseï½œ[çŸ¥ä¹å›ç­”ï½œæ½˜å°å°](https://zhuanlan.zhihu.com/p/173858031) | âœ… Done Â· 2025-10-31 |
| 6 | **T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**ğŸ”‘ | 2020 | å°†æ‰€æœ‰ NLP ä»»åŠ¡éƒ½å¯è½¬åŒ–æˆ Text-to-Text é€šç”¨æ¡†æ¶ï¼ˆå³ä¸ºT5:æ–‡æœ¬åˆ°æ–‡æœ¬ï¼‰ä»»åŠ¡ï¼Œprompt å…ˆé©±ï½œ[çŸ¥ä¹å›ç­”ï½œAndy Yang](https://zhuanlan.zhihu.com/) | âœ… Done Â· 2025-10-31 |
| 7 | **GPT-3: Language Models are Few-Shot Learners**ğŸ”‘ | 2020 | ä» GPT2 çš„ zeroshot åˆ° GPT3 çš„ fewshotï¼Œæ³›åŒ–èƒ½åŠ›ï¼ˆåº”ç”¨æ½œåŠ›ï¼‰ï½œ[ææ²è¯» GPT è®ºæ–‡çš„ç²¾ç®€ç¬”è®°](https://blog.csdn.net/qq_45276194/article/details/136530979) | âœ… Done Â· 2025-11-03 |
| 8 | **Scaling Laws for Neural Language Models**ğŸ”‘ | 2020 | scaling lawâ€”â€”æ¨¡å‹/æ•°æ®/ç®—åŠ›çš„å¹‚å¾‹è§„å¾‹ï¼›æœ€ä¼˜ç­–ç•¥æ˜¯å¤§æ¨¡å‹é…ä¸­ç­‰æ•°æ®é›†â€”â€”ä¸å®Œå…¨æ‹Ÿåˆ | âœ… Done Â· 2025-11-03 |
| 9 | **Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism** | 2020 | è‹±ä¼Ÿè¾¾å¼€å‘çš„åŸºäº pytorchï¼Œæ¨¡å‹æ•°æ®åŒæ—¶å¹¶è¡Œï¼Œè¶…å¤§ LLM å¯è®­ç»ƒçš„æ¶æ„ | âœ… Done Â· 2025-11-05 |
| 10 | **Switch Transformer: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity** | 2021 | æ–°å•ä¸“å®¶æ¿€æ´»ï¼Œé«˜è®­ç»ƒæ•ˆç‡çš„MoEæ¶æ„ | âœ… Done Â· 2025-11-05 |
| 11 | **Chinchilla: Training Compute-Optimal Large Language Models** | 2022 | data scaling lawâ€”â€”å¤§æ¨¡å‹é…æ›´å¤§æ•°æ®æ˜¯æœ€ä¼˜ç­–ç•¥ | âœ… Done Â· 2025-11-05 |
| 12 | **PaLM: Scaling Language Modeling with Pathways** | 2022 | Google åœ¨è‡ªå®¶ TPU ä¸Šè®­å‡ºçš„ 540B è¶…å¤§ Transformer è¯­è¨€æ¨¡å‹ | âœ… Done Â· 2025-11-05 |
| 13 | **InstructGPT: Training Language Models to Follow Instructions with Human Feedback** | 2022 |  | ğŸ“– Planned |
| 14 | **FLAN / Flan-T5: Scaling Instruction-Finetuned Language Models** | 2022 |  | ğŸ“– Planned |
| 15 | **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** | 2022 |  | ğŸ“– Planned |
| 16 | **Constitutional AI: Harmlessness from AI Feedback** | 2022 |  |  |
| 17 | **GPT-4 Technical Report** | 2023 |  |  |
| 18 | **LLaMA / LLaMA 2: Open and Efficient Foundation Language Models** | 2023 |  |  |
| 19 | **Mixtral: Sparse Mixture-of-Experts Model** | 2024 |  |  |
| 20 | **LLaMA 3 Series** | 2024 |  |  |

---

## ğŸ§± Template for New Logs

| No. | Paper | Year | Keywords / Main Contribution | Status |
|:---:|:------|:----:|:-----------------------------|:--------|
| XX | **[Paper Title]** | [Year] | [Brief summary, <mark>core ideas<mark>, or keywords] | âœ… Done / ğŸ“– Planned / ğŸ•’ Reading + Date |

```
# how to highlight
<mark>highlight text<mark>
```

## ğŸ§© Legend

| Symbol | Meaning |
|:-------|:---------|
| âœ… Done | Fully read and summarized |
| ğŸ“– Planned | Planned for future reading |
| ğŸ”‘ Core | Foundational or highly influential paper |




